{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Pattern Mining\n",
    "\n",
    "Since the dataset is not designed for Apriori and FP-Growth, I do not mine the origin value, but the tuple of (column name, value), because the same value on different columns are different.\n",
    "\n",
    "## Environment\n",
    "- Compatible for both python2 and python3, except that on CSUG machines, there are no pandas module for python3.\n",
    "- All the runtime is tested based on Intel i7-7700k CPU.\n",
    "\n",
    "## Apriori and improvement\n",
    "- apriori.py is the original apriori algorithm in the textbook.\n",
    "- improve.py is the improved apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import apriori\n",
    "import improve\n",
    "import pandas as pd\n",
    "import time\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"adult.data\", sep = \", \", header = None, engine = \"python\")\n",
    "df.columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\\\n",
    "        \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\\\n",
    "        \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"divide\"]\n",
    "\n",
    "min_sup = len(df) * 0.6 # 60% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We here set load the data using pandas, and set the minimum support as 60%\n",
    "- Now begin the Apriori on textbook. It is implemented exactly from the pseudo code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "Runtime: 7.01 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "L = apriori.apriori(df, min_sup = min_sup)\n",
    "res = reduce((lambda x, y: x + y), L) # It's just a shortcut for concat all lists\n",
    "res = list(set(res))\n",
    "print(len(res))\n",
    "print(\"Runtime:\", round(time.time() - start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 34 frequent itemsets. The runtime is around 7 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "#This part is just validating. You can skip this, or paste it elsewhere.\n",
    "#if l and res have the same length, \n",
    "#then all the frequent pattern generated are correct. (assume)\n",
    "l = 0\n",
    "for each in res:\n",
    "    cnt = 0\n",
    "    for _, t in df.iterrows():\n",
    "        if all([t[x[0]] == x[1] for x in each]):\n",
    "            cnt += 1\n",
    "    if cnt > min_sup: l += 1\n",
    "\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('capital-gain', 0),\n",
       "  ('divide', '<=50K'),\n",
       "  ('native-country', 'United-States')),\n",
       " (('capital-loss', 0), ('divide', '<=50K'), ('race', 'White')),\n",
       " (('capital-loss', 0), ('race', 'White')),\n",
       " (('capital-gain', 0),),\n",
       " (('capital-gain', 0), ('divide', '<=50K'), ('race', 'White')),\n",
       " (('capital-loss', 0), ('workclass', 'Private')),\n",
       " (('capital-gain', 0), ('native-country', 'United-States'), ('race', 'White')),\n",
       " (('capital-gain', 0), ('native-country', 'United-States')),\n",
       " (('native-country', 'United-States'), ('workclass', 'Private')),\n",
       " (('race', 'White'),),\n",
       " (('native-country', 'United-States'), ('race', 'White')),\n",
       " (('capital-loss', 0),),\n",
       " (('sex', 'Male'),),\n",
       " (('divide', '<=50K'), ('native-country', 'United-States')),\n",
       " (('capital-gain', 0),\n",
       "  ('capital-loss', 0),\n",
       "  ('native-country', 'United-States'),\n",
       "  ('race', 'White')),\n",
       " (('capital-loss', 0), ('native-country', 'United-States'), ('race', 'White')),\n",
       " (('workclass', 'Private'),),\n",
       " (('capital-gain', 0), ('divide', '<=50K')),\n",
       " (('capital-loss', 0), ('native-country', 'United-States')),\n",
       " (('capital-loss', 0), ('divide', '<=50K')),\n",
       " (('capital-loss', 0),\n",
       "  ('divide', '<=50K'),\n",
       "  ('native-country', 'United-States')),\n",
       " (('capital-gain', 0), ('workclass', 'Private')),\n",
       " (('capital-gain', 0), ('capital-loss', 0), ('workclass', 'Private')),\n",
       " (('capital-gain', 0), ('capital-loss', 0)),\n",
       " (('capital-gain', 0), ('sex', 'Male')),\n",
       " (('capital-gain', 0), ('capital-loss', 0), ('divide', '<=50K')),\n",
       " (('capital-loss', 0), ('sex', 'Male')),\n",
       " (('capital-gain', 0),\n",
       "  ('capital-loss', 0),\n",
       "  ('native-country', 'United-States')),\n",
       " (('native-country', 'United-States'),),\n",
       " (('divide', '<=50K'),),\n",
       " (('divide', '<=50K'), ('race', 'White')),\n",
       " (('capital-gain', 0),\n",
       "  ('capital-loss', 0),\n",
       "  ('divide', '<=50K'),\n",
       "  ('native-country', 'United-States')),\n",
       " (('capital-gain', 0), ('race', 'White')),\n",
       " (('capital-gain', 0), ('capital-loss', 0), ('race', 'White'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now the improved apriori.\n",
    "- We know that the textbook version of apriori keeps scanning all the dataset. Every time we generate frequent k-itemsets, we scan the dataset, which is very costy. The improved version only scan the dataset once. All the work is done in find_freq_1_itemset. The first scan may take longer time, but the work later takes nearly no time.\n",
    "- The tricky part here is, we keep a record of the line number of each occurance, so that when generating k-itemsets from (k-1)-itemsets, we only need to get the intersection of the line numbers instead of scan the whole dataset.\n",
    "\n",
    "    For example:<br />\n",
    "    i  X Y<br />\n",
    "    1 2 4<br />\n",
    "    2 3 4<br />\n",
    "    3 3 4<br />\n",
    "    4 5 5<br />\n",
    "    5 5 6<br />\n",
    "    \n",
    "    Here, i is line number, X Y are two different attributes. Let's say the minimum support is 2, so the first scan may generate 1-itemset: {(X, 3)}, {(X, 5)}, {(Y, 4)}. Also, the occurance line number are generated. For {(X, 3)}, it is {2,3}, for {(X, 5)} it's {4,5}, for {(Y, 4)} it's {1,2,3}. And the length of line number set is the support count.\n",
    "    When generating frequent 2-itemsets, we do not need to scan the dataset again. We simply generate the intersection of each two of the line number sets. At last, we have {(X, 3), (Y, 4)} with line number set {2,3}. This is the end of mining, and all the frequent itemsets are: {(X, 3)}, {(X, 5)}, {(X, 3), (Y, 4)}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "Runtime: 4.94 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "L = improve.apriori(df, min_sup = min_sup)\n",
    "res = reduce((lambda x, y: x + y), L)\n",
    "res = list(set(res))\n",
    "print(len(res))\n",
    "print(\"Runtime:\", round(time.time() - start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The time has decreased. Now let's compare the two algorithms with different min_sup. Let's set it as 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Original Apriori Runtime: 3.49 seconds.\n",
      "8\n",
      "Improved Apriori Runtime: 4.7 seconds.\n"
     ]
    }
   ],
   "source": [
    "min_sup = len(df) * 0.8\n",
    "start = time.time()\n",
    "L = apriori.apriori(df, min_sup = min_sup)\n",
    "res = reduce((lambda x, y: x + y), L) # It's just a shortcut for concat all lists\n",
    "res = list(set(res))\n",
    "print(len(res))\n",
    "print(\"Original Apriori Runtime:\", round(time.time() - start, 2), \"seconds.\")\n",
    "\n",
    "start = time.time()\n",
    "L = improve.apriori(df, min_sup = min_sup)\n",
    "res = reduce((lambda x, y: x + y), L)\n",
    "res = list(set(res))\n",
    "print(len(res))\n",
    "print(\"Improved Apriori Runtime:\", round(time.time() - start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The minimum support is bigger, so there are no advantage for this improved apriori. However, when the minimum support gets smaller, the time cost of improved apriori will be much less than the original one. In the comparison, the improved apriori is only 0.3 seconds slower when the min_sup is set to 60%. On the contrary, the original apriori is about twice slower, owing to too many times of scan on the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP-Growth\n",
    "- I only realized the skeleton of this algorithm, but there always are some missing patterns no matter how I modify the program.\n",
    "- The pseudo code in the textbook is too obscure for me. I'd rather call it FP-Grose. It is in the fpgrose.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fpgrose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Runtime: 3.08 seconds.\n"
     ]
    }
   ],
   "source": [
    "# min support of 80%\n",
    "min_sup = len(df) * 0.8\n",
    "start = time.time()\n",
    "fp = set(fpgrose.fp_growth(df, min_sup))\n",
    "print(len(fp))\n",
    "print(\"Runtime:\", round(time.time() - start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('capital-gain', 0),),\n",
       " (('capital-gain', 0), ('capital-loss', 0)),\n",
       " (('capital-gain', 0), ('native-country', 'United-States')),\n",
       " (('capital-loss', 0),),\n",
       " (('capital-loss', 0), ('native-country', 'United-States')),\n",
       " (('capital-loss', 0), ('race', 'White')),\n",
       " (('native-country', 'United-States'),),\n",
       " (('race', 'White'),)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Runtime: 3.02 seconds.\n"
     ]
    }
   ],
   "source": [
    "# min support of 60%\n",
    "min_sup = len(df) * 0.6\n",
    "start = time.time()\n",
    "fp = set(fpgrose.fp_growth(df, min_sup))\n",
    "print(len(fp))\n",
    "print(\"Runtime:\", round(time.time() - start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The algorithm is good when the min_sup is 80%, but will miss some itemsets when the min_sup is 60%.\n",
    "- Although the results are missing, the algorithm is mostly completed.\n",
    "- We implement a tree class, which records the name of the treenode, the count on it, and its prefix path, and its children as well. \n",
    "- Also, a node_link is implemented to store the references of each tree node.\n",
    "- A recursive tree construction function is designed to generate conditional fp-trees until a tree with single path occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
