{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Pattern Mining\n",
    "\n",
    "Since the dataset is not designed for Apriori and FP-Growth, I do not mine the origin value, but the tuple of (column name, value), because the same value on different columns are different.\n",
    "\n",
    "## Environment\n",
    "- Compatible for both python2 and python3, except that on CSUG machines, there are no pandas module for python3.\n",
    "- All the runtime is tested based on Intel i7-7700k CPU.\n",
    "\n",
    "## Apriori\n",
    "- apriori.py is the original apriori algorithm in the textbook.\n",
    "- improve.py is the improved apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import apriori\n",
    "import improve\n",
    "import pandas as pd\n",
    "import time\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"adult.data\", sep = \", \", header = None, engine = \"python\")\n",
    "df.columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\\\n",
    "        \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\\\n",
    "        \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"divide\"]\n",
    "\n",
    "min_sup = len(df) * 0.6 # 60% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We here set load the data using pandas, and set the minimum support as 60%\n",
    "- Now begin the Apriori on textbook. It is implemented exactly from the pseudo code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "Runtime: 6.96 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "L = apriori.apriori(df, min_sup = min_sup)\n",
    "res = reduce((lambda x, y: x + y), L) # It's just a shortcut for concat all lists\n",
    "res = list(set(res))\n",
    "print(len(res))\n",
    "print(\"Runtime:\", round(time.time() - start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 34 frequent itemsets. The runtime is around 7 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "#This part is just validating. You can skip this, or paste it elsewhere.\n",
    "#if l and res have the same length, \n",
    "#then all the frequent pattern generated are correct. (assume)\n",
    "l = 0\n",
    "for each in res:\n",
    "    cnt = 0\n",
    "    for _, t in df.iterrows():\n",
    "        if all([t[x[0]] == x[1] for x in each]):\n",
    "            cnt += 1\n",
    "    if cnt > min_sup: l += 1\n",
    "\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('capital-gain', 0),\n",
       "  ('capital-loss', 0),\n",
       "  ('divide', '<=50K'),\n",
       "  ('native-country', 'United-States')),\n",
       " (('capital-gain', 0), ('native-country', 'United-States')),\n",
       " (('capital-gain', 0),\n",
       "  ('capital-loss', 0),\n",
       "  ('native-country', 'United-States')),\n",
       " (('capital-gain', 0), ('race', 'White')),\n",
       " (('capital-gain', 0), ('capital-loss', 0), ('race', 'White')),\n",
       " (('capital-gain', 0), ('divide', '<=50K')),\n",
       " (('capital-gain', 0), ('sex', 'Male')),\n",
       " (('capital-loss', 0), ('sex', 'Male')),\n",
       " (('capital-loss', 0), ('divide', '<=50K')),\n",
       " (('capital-gain', 0),\n",
       "  ('capital-loss', 0),\n",
       "  ('native-country', 'United-States'),\n",
       "  ('race', 'White')),\n",
       " (('capital-loss', 0),),\n",
       " (('capital-loss', 0), ('native-country', 'United-States')),\n",
       " (('capital-gain', 0), ('native-country', 'United-States'), ('race', 'White')),\n",
       " (('capital-gain', 0),\n",
       "  ('divide', '<=50K'),\n",
       "  ('native-country', 'United-States')),\n",
       " (('capital-loss', 0), ('workclass', 'Private')),\n",
       " (('capital-gain', 0), ('workclass', 'Private')),\n",
       " (('capital-loss', 0),\n",
       "  ('divide', '<=50K'),\n",
       "  ('native-country', 'United-States')),\n",
       " (('sex', 'Male'),),\n",
       " (('divide', '<=50K'), ('race', 'White')),\n",
       " (('capital-gain', 0), ('divide', '<=50K'), ('race', 'White')),\n",
       " (('capital-gain', 0), ('capital-loss', 0)),\n",
       " (('divide', '<=50K'), ('native-country', 'United-States')),\n",
       " (('capital-gain', 0),),\n",
       " (('native-country', 'United-States'), ('workclass', 'Private')),\n",
       " (('race', 'White'),),\n",
       " (('divide', '<=50K'),),\n",
       " (('capital-gain', 0), ('capital-loss', 0), ('workclass', 'Private')),\n",
       " (('native-country', 'United-States'), ('race', 'White')),\n",
       " (('native-country', 'United-States'),),\n",
       " (('capital-loss', 0), ('race', 'White')),\n",
       " (('workclass', 'Private'),),\n",
       " (('capital-gain', 0), ('capital-loss', 0), ('divide', '<=50K')),\n",
       " (('capital-loss', 0), ('divide', '<=50K'), ('race', 'White')),\n",
       " (('capital-loss', 0), ('native-country', 'United-States'), ('race', 'White'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Apriori\n",
    "- We know that the textbook version of apriori keeps scanning all the dataset. Every time we generate frequent k-itemsets, we scan the dataset, which is very costy. The improved version only scan the dataset once. All the work is done in find_freq_1_itemset. The first scan may take longer time, but the work later takes nearly no time.\n",
    "- The tricky part here is, we keep a record of the line number of each occurance, so that when generating k-itemsets from (k-1)-itemsets, we only need to get the intersection of the line numbers instead of scan the whole dataset.\n",
    "\n",
    "    For example:\n",
    "    \n",
    "| Line |  X   |  Y   |\n",
    "|------|------|------|\n",
    "|   1  |  2   |  4   |\n",
    "|   2  |  3   |  4   |\n",
    "|   3  |  3   |  4   |\n",
    "|   4  |  5   |  5   |\n",
    "|   5  |  5   |  6   |\n",
    "    \n",
    "    Here, Line is line number, X Y are two different attributes. Let's say the minimum support is 2, so the first scan may generate 1-itemset: {(X, 3)}, {(X, 5)}, {(Y, 4)}. Also, the occurance line number are generated. For {(X, 3)}, it is {2,3}, for {(X, 5)} it's {4,5}, for {(Y, 4)} it's {1,2,3}. And the length of line number set is the support count.\n",
    "    When generating frequent 2-itemsets, we do not need to scan the dataset again. We simply generate the intersection of each two of the line number sets. At last, we have {(X, 3), (Y, 4)} with line number set {2,3}. This is the end of mining, and all the frequent itemsets are: {(X, 3)}, {(X, 5)}, {(X, 3), (Y, 4)}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "Runtime: 4.98 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "L = improve.apriori(df, min_sup = min_sup)\n",
    "res = reduce((lambda x, y: x + y), L)\n",
    "res = list(set(res))\n",
    "print(len(res))\n",
    "print(\"Runtime:\", round(time.time() - start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The time has decreased. Now let's compare the two algorithms with different min_sup. Let's set it as 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Original Apriori Runtime: 3.49 seconds.\n",
      "8\n",
      "Improved Apriori Runtime: 4.69 seconds.\n"
     ]
    }
   ],
   "source": [
    "min_sup = len(df) * 0.8\n",
    "start = time.time()\n",
    "L = apriori.apriori(df, min_sup = min_sup)\n",
    "res = reduce((lambda x, y: x + y), L) # It's just a shortcut for concat all lists\n",
    "res = list(set(res))\n",
    "print(len(res))\n",
    "print(\"Original Apriori Runtime:\", round(time.time() - start, 2), \"seconds.\")\n",
    "\n",
    "start = time.time()\n",
    "L = improve.apriori(df, min_sup = min_sup)\n",
    "res = reduce((lambda x, y: x + y), L)\n",
    "res = list(set(res))\n",
    "print(len(res))\n",
    "print(\"Improved Apriori Runtime:\", round(time.time() - start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The minimum support is bigger, so there are no advantage for this improved apriori. However, when the minimum support gets smaller, the time cost of improved apriori will be much less than the original one. In the comparison, the improved apriori is only 0.3 seconds slower when the min_sup is set to 60%. On the contrary, the original apriori is about twice slower, owing to too many times of scan on the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP-Growth\n",
    "- I only realized the skeleton of this algorithm, but there always are some missing patterns no matter how I modify the program.\n",
    "- The pseudo code in the textbook is too obscure for me, but I have implemented almost all the functions. It is in the fpgrose.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fpgrose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Runtime: 3.04 seconds.\n"
     ]
    }
   ],
   "source": [
    "# min support of 80%\n",
    "min_sup = len(df) * 0.8\n",
    "start = time.time()\n",
    "fp = set(fpgrose.fp_growth(df, min_sup))\n",
    "print(len(fp))\n",
    "print(\"Runtime:\", round(time.time() - start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('capital-gain', 0),),\n",
       " (('capital-gain', 0), ('capital-loss', 0)),\n",
       " (('capital-gain', 0), ('native-country', 'United-States')),\n",
       " (('capital-loss', 0),),\n",
       " (('capital-loss', 0), ('native-country', 'United-States')),\n",
       " (('capital-loss', 0), ('race', 'White')),\n",
       " (('native-country', 'United-States'),),\n",
       " (('race', 'White'),)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Runtime: 3.02 seconds.\n"
     ]
    }
   ],
   "source": [
    "# min support of 60%\n",
    "min_sup = len(df) * 0.6\n",
    "start = time.time()\n",
    "fp = set(fpgrose.fp_growth(df, min_sup))\n",
    "print(len(fp))\n",
    "print(\"Runtime:\", round(time.time() - start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The algorithm is good when the min_sup is 80%, but will miss some itemsets when the min_sup is 60%.\n",
    "- Although the results are missing, the algorithm is mostly completed, and the runtime is to some degree valuable for reference.\n",
    "- We implement a tree class, which records the name of the treenode, the count on it, and its prefix path, and its children as well. \n",
    "- Also, a node_link is implemented to store the references of each tree node.\n",
    "- A recursive tree construction function is designed to generate conditional fp-trees until a tree with single path occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall comparison\n",
    "In this section, we compare the three models, a.k.a. Apriori, FP-Growth, and the improved Apriori. We first test them with different levels of min support, and then with different size of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with min support of 0.2\n",
      "Original Apriori Runtime: 23.99 seconds.\n",
      "FP-Growth Runtime: 3.24 seconds.\n",
      "Improved Apriori Runtime: 35.1 seconds.\n",
      "Testing with min support of 0.3\n",
      "Original Apriori Runtime: 13.97 seconds.\n",
      "FP-Growth Runtime: 3.14 seconds.\n",
      "Improved Apriori Runtime: 10.76 seconds.\n",
      "Testing with min support of 0.4\n",
      "Original Apriori Runtime: 10.06 seconds.\n",
      "FP-Growth Runtime: 3.08 seconds.\n",
      "Improved Apriori Runtime: 6.14 seconds.\n",
      "Testing with min support of 0.6\n",
      "Original Apriori Runtime: 6.98 seconds.\n",
      "FP-Growth Runtime: 3.05 seconds.\n",
      "Improved Apriori Runtime: 4.91 seconds.\n",
      "Testing with min support of 0.8\n",
      "Original Apriori Runtime: 3.43 seconds.\n",
      "FP-Growth Runtime: 2.98 seconds.\n",
      "Improved Apriori Runtime: 4.68 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Different min_sup, from 20% to 80%\n",
    "for portion in [0.2, 0.3, 0.4, 0.6, 0.8]:\n",
    "    min_sup = len(df) * portion\n",
    "    print(\"Testing with min support of\", portion)\n",
    "    start = time.time()\n",
    "    L = apriori.apriori(df, min_sup = min_sup)\n",
    "    print(\"Original Apriori Runtime:\", round(time.time() - start, 2), \"seconds.\")\n",
    "\n",
    "    start = time.time()\n",
    "    fp = set(fpgrose.fp_growth(df, min_sup))\n",
    "    print(\"FP-Growth Runtime:\", round(time.time() - start, 2), \"seconds.\")\n",
    "\n",
    "    start = time.time()\n",
    "    L = improve.apriori(df, min_sup = min_sup)\n",
    "    print(\"Improved Apriori Runtime:\", round(time.time() - start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with min support of 0.2\n",
      "Original Apriori Runtime: 0.08 seconds.\n",
      "FP-Growth Runtime: 0.64 seconds.\n",
      "Improved Apriori Runtime: 1.2 seconds.\n",
      "Testing with min support of 0.4\n",
      "Original Apriori Runtime: 0.13 seconds.\n",
      "FP-Growth Runtime: 1.23 seconds.\n",
      "Improved Apriori Runtime: 2.16 seconds.\n",
      "Testing with min support of 0.6\n",
      "Original Apriori Runtime: 0.17 seconds.\n",
      "FP-Growth Runtime: 1.82 seconds.\n",
      "Improved Apriori Runtime: 3.08 seconds.\n",
      "Testing with min support of 0.8\n",
      "Original Apriori Runtime: 4.09 seconds.\n",
      "FP-Growth Runtime: 2.43 seconds.\n",
      "Improved Apriori Runtime: 3.94 seconds.\n",
      "Testing with min support of 1\n",
      "Original Apriori Runtime: 7.05 seconds.\n",
      "FP-Growth Runtime: 3.05 seconds.\n",
      "Improved Apriori Runtime: 4.97 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Different dataset size, from 20% original size to 100%\n",
    "for portion in [0.2, 0.4, 0.6, 0.8, 1]:\n",
    "    tmp = df[:int(len(df) * portion)]\n",
    "    min_sup = len(df) * 0.6\n",
    "    print(\"Testing with min support of\", portion)\n",
    "    start = time.time()\n",
    "    L = apriori.apriori(tmp, min_sup = min_sup)\n",
    "    print(\"Original Apriori Runtime:\", round(time.time() - start, 2), \"seconds.\")\n",
    "\n",
    "    start = time.time()\n",
    "    fp = set(fpgrose.fp_growth(tmp, min_sup))\n",
    "    print(\"FP-Growth Runtime:\", round(time.time() - start, 2), \"seconds.\")\n",
    "\n",
    "    start = time.time()\n",
    "    L = improve.apriori(tmp, min_sup = min_sup)\n",
    "    print(\"Improved Apriori Runtime:\", round(time.time() - start, 2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Table 1 Rumtime on different levels of min_sup (seconds)\n",
    "\n",
    "|          | 0.2  | 0.3  | 0.4  | 0.6  | 0.8  |\n",
    "|----------|------|------|------|------|------|\n",
    "| Apriori  | 23.99| 13.97| 10.06| 6.98 | 3.43 |\n",
    "| FPGrowth | 3.24 | 3.14 | 3.08 | 3.05 | 2.98 |\n",
    "| Improved | 35.1 | 10.76| 6.14 | 4.91 | 4.68 |\n",
    "\n",
    "- Table 2 Runtime on different size of dataset (seconds)\n",
    "\n",
    "|          | 0.2  | 0.4  | 0.6  | 0.8  | 1    |\n",
    "|----------|------|------|------|------|------|\n",
    "| Apriori  | 0.08 | 0.13 | 0.17 | 4.09 | 7.05 |\n",
    "| FPGrowth | 0.64 | 1.23 | 1.82 | 2.43 | 3.05 |\n",
    "| Improved | 1.2  | 2.16 | 3.08 | 3.94 | 4.97 |\n",
    "\n",
    "- From Table 1, we can see that FPGrowth is always so fast. For original Apriori, its performance is just so so, under most situations very slow. For the improved Apriori, it performs well when the support count is larger than 30%. The sudden rise in its time cost when min_sup is 20% really surprised me, I think it is because the set intersection and union cost too much time.\n",
    "- From Table 2, we can see that the original apriori performs the best when dataset size is small, but the worst when the size increases to a big number. Both FPGrowth and improved Apriori take less time when dealing with large datasets. The two algorithms need to execute more in the beginning, so they are slower than original Apriori when dealing with a small dataset. They sacrifice space to win time efficiency.\n",
    "- Above all, the improved Apriori indeed improved the original Apriori when the min_sup is not too small. FPGrowth is the least sensitive to min_sup levels. Original Apriori is the most sensitive to dataset sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
